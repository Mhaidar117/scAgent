name: Evaluation

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run evaluation nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      tags:
        description: 'Tags to filter (comma-separated)'
        required: false
        default: 'basic,qc'
      include_optional:
        description: 'Include optional tests'
        required: false
        type: boolean
        default: false

jobs:
  eval-basic:
    name: Basic Evaluation
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,qc]"
    
    - name: Run basic evaluation
      run: |
        python -m scqc_agent.eval.runner \
          --prompts eval/prompts.yaml \
          --output eval_results_basic.json \
          --tags basic,qc \
          --no-optional
    
    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: eval-results-basic-py${{ matrix.python-version }}
        path: eval_results_basic.json
        retention-days: 30

  eval-comprehensive:
    name: Comprehensive Evaluation
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-comprehensive-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-comprehensive-
          ${{ runner.os }}-pip-
    
    - name: Install full dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,qc,models]"
    
    - name: Run comprehensive evaluation
      run: |
        python -m scqc_agent.eval.runner \
          --prompts eval/prompts.yaml \
          --output eval_results_comprehensive.json \
          ${{ github.event.inputs.tags && format('--tags {0}', github.event.inputs.tags) || '' }} \
          ${{ github.event.inputs.include_optional == 'false' && '--no-optional' || '' }}
    
    - name: Upload comprehensive results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: eval-results-comprehensive
        path: eval_results_comprehensive.json
        retention-days: 90

  eval-quality-gates:
    name: Quality Gates Test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,qc]"
    
    - name: Test quality assertions
      run: |
        python -c "
        from scqc_agent.quality.assertions import *
        import tempfile
        import numpy as np
        
        try:
            import scanpy as sc
            import anndata as ad
            
            # Test with valid data
            data = np.random.poisson(5, (100, 50))
            adata = ad.AnnData(data.astype(float))
            adata.obs['n_genes_by_counts'] = (data > 0).sum(1)
            adata.obs['total_counts'] = data.sum(1)
            adata.obs['pct_counts_mt'] = np.random.uniform(0, 20, 100)
            
            assert_qc_fields_present(adata)
            assert_pct_mt_range(adata)
            
            print('✅ Quality gates working correctly')
        except ImportError:
            print('⚠️ Scanpy not available, skipping quality gate tests')
        "
    
    - name: Test edge cases
      run: |
        python -c "
        from scqc_agent.quality.assertions import QualityGateError
        
        try:
            import scanpy as sc
            import anndata as ad
            import numpy as np
            
            # Test invalid data
            data = np.random.poisson(5, (100, 50))
            adata = ad.AnnData(data.astype(float))
            adata.obs['pct_counts_mt'] = np.random.uniform(95, 105, 100)  # Invalid range
            
            try:
                from scqc_agent.quality.assertions import assert_pct_mt_range
                assert_pct_mt_range(adata)
                raise AssertionError('Should have failed')
            except QualityGateError:
                print('✅ Quality gate correctly caught invalid data')
                
        except ImportError:
            print('⚠️ Scanpy not available, skipping edge case tests')
        "

  report-status:
    name: Report Evaluation Status
    runs-on: ubuntu-latest
    needs: [eval-basic, eval-quality-gates]
    if: always()
    
    steps:
    - name: Download evaluation results
      uses: actions/download-artifact@v3
      with:
        pattern: eval-results-*
        merge-multiple: true
    
    - name: Report status
      run: |
        echo "## Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f eval_results_basic.json ]; then
          python -c "
          import json
          
          with open('eval_results_basic.json') as f:
              results = json.load(f)
          
          print(f'### Basic Evaluation')
          print(f'- **Pass Rate**: {results[\"pass_rate\"]:.1%}')
          print(f'- **Tests**: {results[\"passed_prompts\"]}/{results[\"total_prompts\"]} passed')
          print(f'- **Time**: {results[\"total_execution_time\"]:.1f}s')
          
          if results['pass_rate'] >= 0.95:
              print('✅ **Status**: PASSING')
          else:
              print('❌ **Status**: FAILING')
              
          if results['failed_prompts'] > 0:
              print('')
              print('**Failed Tests:**')
              for result in results['results']:
                  if not result['passed']:
                      print(f'- {result[\"prompt_id\"]}: {result.get(\"error_message\", \"Criteria not met\")}')
          " >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ No basic evaluation results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Quality Gates" >> $GITHUB_STEP_SUMMARY
        echo "✅ Quality assertion tests passed" >> $GITHUB_STEP_SUMMARY
        
        # Set job status based on results
        if [ -f eval_results_basic.json ]; then
          PASS_RATE=$(python -c "import json; print(json.load(open('eval_results_basic.json'))['pass_rate'])")
          if (( $(echo "$PASS_RATE < 0.95" | bc -l) )); then
            echo "❌ Evaluation failed: pass rate $PASS_RATE below 95% threshold"
            exit 1
          fi
        fi
